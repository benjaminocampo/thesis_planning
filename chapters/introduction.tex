\chapter{Introducción}
\label{ch:into}

\section{Contexto y motivación}
La planificación automática, o simplemente \emph{planning}, es una de las áreas
centrales de la inteligencia artificial debido a su extenso uso en dominios,
tales como, control de misiones espaciales \citep{RabideauG-et-al-2001}, manejo
de crisis \citep{Bienkowki-1995}, generación de textos narrativos
\citep{Goudoulakis-et-al-2016}, o robótica \citep{Munoz-et-al-2016}.

El objetivo es definir un modelo que se asemeje a una tarea o problema de
nuestro entorno por medio de una especificación donde se describan todos los
detalles en que esta consiste. Estas son el estado inicial, la meta, y un
conjunto de acciones. El estado inicial describe las propiedades válidas con las
que se parte dentro del entorno. La meta o estado final representa cuáles son
las propiedades deseables en él. Y el conjunto de acciones está compuesto de
transformadores de estados que permiten alterar estas propiedades. Si se obtiene
una secuencia de acciones que sea aplicable en el estado inicial, y que luego de
su ejecución conlleve a la meta, entonces dicha secuencia es considerada un plan
del problema. \citep{Georgievski-et-al-2016}

Para hallar un plan que resuelva la tarea, se realizan técnicas de búsqueda y
optimización, efectuadas por \emph{planificadores}, algoritmos que computan el
comportamiento de un agente por medio de una descripción del problema comúnmente
definida en el \emph{Planning Domain Definition Language} (PDDL). En PDDL, se
especifican las propiedades del entorno en términos de predicados, y las
transformaciones por medio de esquemas de acción. Estas consisten en expresiones
parametrizadas que pueden ser instanciadas por un conjunto de objetos. El
planificador por medio de la especificación, define un espacio de búsqueda sobre
el cual encontrar un plan.
\citep{Georgievski-et-al-2016}

Sin embargo, la mayoría de los planificadores trabajan sobre una representación
sin variables libres. Por consecuente, estos computan todas las instanciaciones
que asignan los objetos a los argumentos de los predicados y esquemas de acción
definidos por el PDDL del problema. Este proceso, conocido como
\emph{grounding}, es exponencial en la cantidad de argumentos de los esquemas de
acción y predicados, llegando a obtener una cantidad inmensurable de instancias
cuando el número de parámetros definidos es alto. Esto puede llevar a la falla
por parte del planificador para resolver la tarea sin antes haber tenido la
posibilidad de realizar la búsqueda en el espacio de instancias, incluso cuando
en la práctica solo una pequeña fracción de ellas ocurren en los planes del
problema.
\citep{Gnad_Torralba_Dominguez_Areces_Bustos_2019}

Ahora bien, si se obtienen las acciones necesarias para confeccionar por lo
menos un plan, entonces el proceso de búsqueda encontraría alguna de tales
soluciones. Por ende, surge la siguiente pregunta ¿cómo determinamos que
acciones son relevantes para algún plan de tal manera que puedan ser incluidas
en el proceso de grounding? Más formalmente, ¿existe alguna función $F$
que dada una acción $A$ y un problema $P$, determine que tan relevante es $A$
para hallar un plan en $P$?

Esta pregunta es las que nos llevó a considerar el uso de técnicas de
\emph{aprendizaje automático}. La idea principal fue lograr encontrar un modelo
que prediga la probabilidad de que una acción sea relevante a partir de planes
de varios problemas y acciones etiquetadas. No obstante, dado que las tareas que
nos interesan resolver son aquellas que no pueden ser groundeadas. Utilizar
problemas equivalentes para construir el material de entrenamiento no es una
posibilidad, ya que no podríamos computar algún plan de la tarea ni determinar
que acciones son relevantes para anotarlas. Aún en los casos en que esto sea
posible, hacerlo es realmente costoso en términos computacionales y de tiempo
necesario. Estas son algunas de las dificultades que nos llevó a utilizar una
aproximación del plan real, conocida como \emph{plan relajado}, y PDDL's de
problemas más sencillos de resolver que permitan guiar el proceso de
\emph{grounding} en otros más complejos.

Por otro lado, los modelos de aprendizaje automático dependen fuertemente de
como se representan los datos que uno tiene disponible \citep{Heaton-2016-AnEA}.
Para obtener un modelo de aprendizaje supervisado se necesita construir un
vector de \emph{features} que permita codificar nuestros datos, en particular,
un plan relajado y una acción. Una opción es proyectar los ejemplos a un espacio
vectorial a partir de una codificación del tipo \emph{one-hot}, donde enumeramos
los posibles esquemas de acción, y objetos de un problema para representar
numéricamente una acción y por consecuente una secuencia de ellas. Otra
posibilidad es utilizar el concepto de \emph{word embeddings} proveniente del
procesamiento del lenguaje natural. Estos tienen como objetivo proyectar
palabras y oraciones a un espacio $N$ dimensional que preserve su semántica, es
decir, expresiones con un significado similar, son dispuestas cerca en el
espacio. \citep{Mikolov-Ilya-Kai-Greg-Jeffrey-2013,
Pennington-Jeffrey-Socher-Richard-Manning-Christopher-2014,
Bojanowski-Grave-Joulin-Mikolov-2016}. La intuición principal es obtener un
modelo de lenguaje, pensando un plan como una oración, que caracterice el
lenguaje generado por los planes relajados y las acciones instanciadas. 

En resumen, la hipótesis inicial de este trabajo es investigar distintas
codificaciones de nuestros datos que permitan al modelo de \emph{machine
learning} reconocer la relación entre los planes relajados, acciones, y planes
reales, de tal manera que guíen el proceso de \emph{grounding}.

\section{Trabajos relacionados}

En la literatura de planning se pueden encontrar numerosas técnicas que intentan
lidiar con el proceso de grounding efectuado por el planificador. Algunas se
basan en evitar procesar acciones que no son alcanzables relajadamente desde el
estado inicial, tal es el caso del planificador Fast Downward
\citep{Helmert-2011} utilizado en competencias de planning. En
\citep{Röger_Sievers_Katz_2018} estudian nociones de simetría sobre la
especificación de la tarea de planning para evitar trabajo redundante. Otra
opción es reducir el tamaño de las interfaces de los esquemas de acción al
dividirla en subpartes. De esta manera la explosión exponencial de acciones
groundeadas se disminuye drásticamente. Esta técnica se denomina \emph{action
schema splitting} y es desarrollada en
\citep{Areces_Bustos_Dominguez_Hoffmann_2014}. También existen métodos que
evitan el proceso de grounding completamente y realizan la búsqueda del plan a
partir de su especificación PDDL \citep{Penberthy-1992}.

En particular, esta tesis sigue la continuación de un grupo de heurísticas
desarrolladas en \citep{Gnad_Torralba_Dominguez_Areces_Bustos_2019} que proponen
realizar la búsqueda de una solución a una tarea de planning por medio de un
espacio reducido utilizando técnicas de machine learning, pero con un material de
entrenamiento generado por reglas o propiedades que un plan relajado posee en
conjunto a los estados iniciales y finales del problema en cuestión.

Salvo esta última heurística, todas utilizan información de la tarea totalmente
procesada para luego determinar el subconjunto de acciones sobre el cual
realizar la búsqueda. Es por eso que resultan descartadas en aquellas
situaciones donde grounding total es inviable. Tampoco se han vistos muchos
estudios sobre el uso de técnicas de aprendizaje automático en el área de
planning, lo cual es una fuerte iniciativa para empezar a combinar estos dos
campos.

\section{Estructura de la tesis}

Los capítulos \ref{ch:lit_plannin} y \ref{ch:lit_ml} detallan los conceptos
fundamentales de planning y aprendizaje automático que se utilizaran para unir
estas dos áreas y sobre que problemática estamos trabajando.

El capítulo 4 describe el conjunto de entrenamiento y test que se utilizaron,
así como su preprocesamiento y codificación. También se dejará en evidencia la
dificultad de los problemas que un planificador no puede resolver debido al
problema de grounding y sobre los cuales nos interesa aplicar grounding
heurístico.

El capítulo 5 presenta los experimentos desarrollados, especificando el
preprocesamiento de datos realizados, los algoritmos de machine learning, y los
resultados obtenidos para finalmente compararlos entre sí. Además, se mencionan
otros experimentos que fueron parte del proceso científico de este trabajo y que
a pesar de no lograr un resultado esperado, fueron de gran utilidad para la
reevaluación de nuestros objetivos.

Por último, en el capítulo 6 se escriben las conclusiones obtenidas a partir de
los experimentos junto a las líneas de trabajo a futuro.