\chapter{Introducción}
\label{ch:into}

La planificación automática, o simplemente \emph{Planning}, es una de las áreas
centrales de la inteligencia artificial debido a su extenso uso en dominios,
tales como, control de misiones espaciales \citep{RabideauG-et-al-2001}, manejo
de crisis \citep{Bienkowki-1995}, generación de textos narrativos
\citep{Goudoulakis-et-al-2016}, o robótica \citep{Munoz-et-al-2016}.

El objetivo es definir un modelo que se asemeje a una tarea o problema de
nuestro entorno por medio de una especificación donde se describan todos los
detalles en que esta consiste. Estas son el estado inicial, la meta, y un
conjunto de acciones. El estado inicial describe las propiedades válidas
iniciales dentro del entorno. La meta o estado final representa cuales son las
propiedades deseables en él. Y el conjunto de acciones está compuesto de
transformadores de estados que permiten alterar estas propiedades. Si se obtiene
una secuencia de acciones que sea aplicable en el estado inicial, y que luego de
su ejecución conlleve a la meta, entonces dicha secuencia es considerada un plan
del problema. \citep{Georgievski-et-al-2016}

Para hallar un plan que resuelva la tarea, se realizan técnicas de búsqueda y
optimización, efectuadas por \emph{planners}, algoritmos que computan el
comportamiento de un agente por medio de una descripción del problema comunmente
definida en el \emph{Planning Domain Definition Language} (PDDL). En PDDL, se
especifican las propiedades del entorno en términos de predicados, y las
transformaciones por medio de esquemas de acción. Estas consisten en expresiones
parametrizadas que pueden ser instanciadas por un conjunto de objetos. El
\emph{planner} por medio de la especificación, define un espacio de búsqueda
cuya dificultad de encontrar un plan aumenta con su complejidad.
\citep{Georgievski-et-al-2016}

Sin embargo, la mayoría de los \emph{planners} trabajan sobre una representación
sin variables libres. Por consecuente, estos computan todas las instanciaciones
que asignan los objetos a los argumentos de los predicados y esquemas de acción
definidas por el PDDL del problema. Este proceso, conocido como
\emph{grounding}, es exponencial en la cantidad de argumentos de los esquemas de
acción y predicados, llegando a obtener una cantidad inmensurable de instancias
cuando el número de parámetros definidos es alto. Esto puede llevar a la falla
por parte del \emph{planner} para resolver la tarea sin antes haber tenido la
posibilidad de realizar la búsqueda en el espacio de instancias, incluso cuando
en la práctica solo una pequeña fracción de estas instancias ocurren en los
planes del problema. \citep{Gnad_Torralba_Dominguez_Areces_Bustos_2019}

Ahora bien, si se instancian las acciones necesarias para confeccionar por lo
menos un plan, entonces el proceso de búsqueda encontraría alguna de tales
soluciones \textcolor{red}{TODO CITAR}. Por ende, surge la siguiente pregunta
¿cómo determinamos que acciones son relevantes para algún plan de tal manera que
puedan ser incluidas en el proceso de \emph{grounding}? Más formalmente, ¿existe
alguna función $F$ que dada una acción $A$ y un problema $P$, determine que tan
relevante es $A$ para hallar un plan en $P$?

Esta pregunta es las que nos llevó a considerar el uso de técnicas de
\emph{machine learning}. La idea principal fue lograr encontrar un modelo que
prediga la probabilidad de que una acción sea relevante a partir de planes de
varios problemas y acciones etiquetadas. No obstante, dado que las tareas que
nos interesa resolver son aquellas que no pueden ser instanciadas. Utilizar
problemas equivalentes para construir el material de entrenamiento no es una
posibilidad, ya que no podríamos computar algún plan de la tarea ni determinar
que acciones son relevantes para anotarlas. Aún en los casos en que esto sea
posible, hacerlo es realmente costoso en términos computacionales y de tiempo
necesario \textcolor{red}{TODO CITAR}. Estas son algunas de las dificultades que
nos llevó a utilizar una aproximación del plan real, conocida como \emph{relaxed
plan} o plan relajado, y PDDL's de problemas más sencillos de resolver que
permitan guiar el proceso de \emph{grounding} en otros más complejos.

Por otro lado, los modelos de \emph{machine learning} dependen fuertemente de
como se representan los datos que uno tiene disponible \citep{Heaton-2016-AnEA}.
Para obtener un modelo de aprendizaje supervisado se necesita construir un
vector de \emph{features} que permita codificar nuestros datos, en particular,
un plan relajado y una acción. \textcolor{blue}{WIP (No hablar de Word
Embeddings solo de la codificación custom)} \textcolor{red}{Una posibilidad es
utilizar el concepto de \emph{word embeddings} proveniente del procesamiento del
lenguaje natural. Estos tienen como objetivo proyectar palabras y oraciones a un
espacio $N$ dimensional que preserve la semántica de las palabras, es decir,
expresiones con un significado similar, son dispuestas cerca en el espacio.
\citep{Mikolov-Ilya-Kai-Greg-Jeffrey-2013,
Pennington-Jeffrey-Socher-Richard-Manning-Christopher-2014,
Bojanowski-Grave-Joulin-Mikolov-2016}. La intuición principal es obtener un
modelo de lenguaje, pensando un plan como una oración, que caracterice el
lenguaje generado por los planes relajados y las acciones instanciadas.}

En resumen, la hipótesis inicial de este trabajo es investigar distintas
codificaciones \textcolor{red}{a partir de \emph{word embeddings}} que permitan
al modelo de \emph{machine learning} reconocer la relación entre los planes
relajados, acciones instanciadas, y los planes reales, de tal manera que guíen
el proceso de \emph{grounding}.