\chapter{Aprendizaje automático}
\label{ch:lit_ml}

En este capítulo se profundizará en el área de \emph{aprendizaje automático}
abarcando la representación de palabras y oraciones dadas por codificaciones del
tipo one-hot, word embeddings, conceptos de aprendizaje supervisado y no
supervisado, modelos de clasificación, y métricas. Este capítulo se baso en
mayor parte del contenido en \citep{Bishop-2006}, y \citep{Tianqi-2016} para la
explicación de conceptos de aprendizaje supervisado, y
\citep{Bojanowski-Grave-Joulin-Mikolov-2016, bojanowski-2017, Mikolov-2013} para
representación de palabras.

\section{Orígenes y evolución}

El aprendizaje automático es el campo de la inteligencia artificial que busca
desarrollar programas que mejoren en base a la experiencia, automatizar el
reconocimiento de patrones estadísticos sobre volúmenes de datos, y utilizar
algoritmos de computación que tomen acción sobre datos no aprendidos.

Por moderno que pueda parecer este campo, tuvo sus comienzos en los años 50 a
partir del famoso \emph{Test de Turing}, una máquina cuyo objetivo era engañar a
un humano haciéndole creer que se encontraba delante de otra persona en lugar de
un ordenador, llegando  a la conclusión de que los computadores de propósito
general, podrían ser capaces de aprender y ser de alguna manera originales.
\citep{test-turing-website}

En los últimos años varias aplicaciones se han beneficiado de esta área, desde
programas relacionados con la detección fraudulenta de transacciones con tarjeta
de crédito \citep{Fang-2021}, sistemas de recomendación que guían usuarios en un
servicio de acuerdo a sus preferencias \citep{Burke-2007}, o incluso vehículos
que se manejan sin necesidad de la intervención del conductor
\citep{Sorin-2019}. Incluso en algúnas tareas de reconocimiento de patrones el
aprendizaje automático ha logrado obtener tazas menores al error húmano
\citep{niklas-et-al-2020}. Al mismo tiempo, una importante cantidad de avances
teóricos y algorítmicos se fueron realizando formando las bases de este campo. 

\section{Aprendizaje supervisado}

El aprendizaje supervisado es una subárea del aprendizaje automático cuyo
objetivo es deducir un modelo a partir de datos anotados que permita mapear
ejemplos no vistos previamente. Esta información está compuesta por un conjunto
de ejemplares y sus correspondientes etiquetas. En ocasiones, dada por un
anotador humano que indica el resultado esperado del modelo a partir del dato
como entrada. Las etiquetas pueden ser categóricas o continuas determinando un
problema de clasificación o regresión, respectivamente. Algunas tareas más
frecuentes de clasificación son la categorización de documentos
\citep{lulu-2019}, reconocimiento de lenguaje ofensivo \citep{bencheng-2021}, o
análisis de sentimiento \citep{Nhan-2020}. Mientras que para regresión, lo son
las estimaciones de precios de artículos, objetos, o viviendas \citep{Yeh-2011}.

El resultado de ejecutar un algoritmo de machine learning supervisado se puede
expresar como una función $f(\vect{x})$ que recibe un ejemplar $\vect{x}$ como
entrada y genera su predicción $y$ de salida. Las muestras utilizadas para
ajustar $f$ están dadas por pares (vector, etiqueta) $\{(\vect{x}_1, y_1), ...,
(\vect{x}_n, y_n)\}$ conformando el \emph{conjunto de entrenamiento}.

La forma precisa de $f$ es determinada durante la fase de entrenamiento. Una vez
transcurrida, se puede estimar la etiqueta de nuevos datos que no pertenezcan al
conjunto de entrenamiento. En particular, se suele medir la performance del
modelo comparando las predicciones con la etiqueta real de nuevos datos que no
fueron parte utilizados en la fase de entrenamiento. En el caso que la
predicción se aproxime a la esperada para estas nuevas entradas, entonces el
modelo habría logrado generalizar la tarea.

Sin embargo, los modelos de aprendizaje supervisado están limitados por la
disponibilidad de las anotaciones y la dificultad para obtenerlas. Algunas
tareas son relativamente sencillas de etiquetar por cualquier persona, (e.g.,
como determinar si en una foto aparece un gato), mientras que en otros puede
llegar a requerir humanos que sean expertos de dominio (e.g., abogados, médicos,
lingüistas, etc).

\section{Aprendizaje no supervisado}

En contraste al método anterior, el aprendizaje no supervisado consiste en
descubrir automáticamente patrones sobre los datos de entrenamiento que permitan
explicarlos sin depender de datos anotados. Es considerada como parte del área
del análisis y exploración. Es por eso que no pueden ser evaluados basándose en
exactitud o precisión, si no más bien en la cantidad de información que podamos
extraer de los datos a partir del uso de estas técnicas. Algo a considerar es la
facilidad para disponer de estos datos (en comparación a los no supervisados) al
no requerir ningún tipo de anotación previa.
Un ejemplo de uso de aprendizaje no supervisado es la segmentación de mercado en
aplicaciones e-commerce. En este se buscan desarrollar estrategias de mercado a
partir de la identificación de similitudes entre clientes por medio de sus
necesidades, y preferencias \citep{Tiwari-2018}. Otro ejemplo muy común es en la
selección de características y reducción de dimensionalidad, permitiendo eliminar
información de nuestro conjunto de datos que es irrelevante y que afecta de
manera negativa la eficiencia y efectividad de algoritmos de aprendizaje
automático \citep{Farahat-2013}.

\section{Algoritmos de clasificación}
\label{lit:algorithms}

Comenzaremos estudiando algunos métodos de aprendizaje supervisado sin
preocuparnos inicialmente por la manera en que los ejemplares están codificados
con excepción de las etiquetas. Es decir para un conjunto de entrenamiento
$\{(\vect{x}_1, y_1), ..., (\vect{x}_n, y_n)\}$ la codificación de los
$\vect{x}_i's$ será irrelevante por el momento. La únicas restricciones que se
pedirán es que sean vectores numéricos. Los $y_i's$ aparte de ser numéricos, deben
ser categóricos, dado que trataremos con algoritmos de clasificación.

El objetivo principal de un clasificador es aproximar una función no conocida
$f$ a partir de un modelo $h: \mathbb{R}^D \rightarrow \{c_1,.., c_k\}$ la cual
mapea ejemplos de entrada a una categoría $c_j$. En particular, nos enfocaremos
en clasificadores binarios $k = 2$ que fueron los utilizados para la realización
de este trabajo.

\subsection{Modelos lineales}

La forma más sencilla de representar un función que discrimine datos de entrada
en dos clases es a partir de una función lineal que dependa de estos datos:

\begin{align}
    h_{\vect{w}, w_0}\left( \vect{x} \right) &= \vect{w}^{\top} \vect{x} + w_0  \label{eq:linear_w_bias}\\
                                           &= \sum_{j = 1 }^{D} w_j x_j + w_0 \label{eq:linear_wo_bias}
\end{align}

Donde $\vect{w}$ es llamado el \emph{vector de pesos}, y $w_0$ es el
\emph{sesgo}. Ambos son parámetros de la función $h$ y son los que se buscan
ajustar para lograr separar el conjunto de datos donde $\vect{x}$ es uno de
tales ejemplos. Las dimensiones de los vectores $\vect{x}$, y $\vect{w}$ son
ambas $(D \times 1)$ a diferencia del sesgo que es un escalar. La expresión
$\vect{w}^{\top} \vect{x}$ es el producto interno entre $\vect{w}$ y $\vect{x}$.
Por lo que el resultado de $\vect{w}^{\top} \vect{x} + b$ es también un escalar.
Dado que nos interesa asignar a $\vect{x}$ una categoría, se debe decidir a cual
pertenece en base al resultado obtenido por el producto interno. Es por eso que
surge la necesidad de agregar una nueva operación que discrimine un valor de
entrada en una clase. Para ello definimos una \emph{función de decisión} $d:
\mathbb{R} \rightarrow \{c_1, c_2\}$ tal que mapea valores a una clase $c_1$, o
$c_2$.

Una posible función de decisión podría estar definida como $d(t) = c_1$ si $t
\geq 0$, $d(t) = c_2$ caso contrario. Por lo tanto la frontera de decisión queda
definida por la relación $h_{\vect{w}, w_0}\left( \vect{x} \right) = 0$ y
corresponde a un hiperplano de ($D-1$ dimensiones) en uno de $D$ dimensiones. El
sesgo $w_0$ desplaza el hiperplano del origen de coordenadas de $D$. Un ejemplo de
pares de puntos separados por un hiperplano (en este caso una recta) se muestra
en la figura \ref{fig:linear_model_boundary}.

Algunas veces es conveniente usar una notación más compacta en la que
introducimos una ``inocente`` componente de entrada más $x_0 = 1$ al vector
$\vect{x}$ para luego definir $\vect{w}' = (w_0, \vect{w})$ y $\vect{x}' = (x_0,
\vect{x})$. Luego las ecuaciones \ref{eq:linear_w_bias} y
\ref{eq:linear_wo_bias} se pueden expresar como:

\begin{figure}
    \centering
    \includegraphics[scale=0.5]{figures/decision_boundary.png}
    \caption{Frontera de decisión de un modelo lineal separando los puntos que
    tienen etiqueta $c_1$ = 1 y $c_2$ = -1}.
    \label{fig:linear_model_boundary}
\end{figure}

\begin{align}
    h_{\vect{w'}}\left( \vect{x'} \right) &= \vect{w'}^{\top} \vect{x'}\\
                                           &= \sum_{j = 0 }^{D} w_j x_j
\end{align}

Ahora la frontera de decisión corresponde a un hiperplano de dimensión $D$ que
pasa por el origen del espacio $D + 1$ dimensional. A partir de ahora, cada vez
que se omita el sesgo estaremos utilizando esta notación.

\subsection{Modelos probabilísticos}
\label{subch:prob_models}

En el capítulo \ref{ch:lit_planning} mencionamos que el objetivo de usar un
algoritmo de aprendizaje automático es determinar una función que modele la
probabilidad de que una acción sea relevante para encontrar un plan de una tarea
STRIPS. Sin embargo, desconocemos aún de que manera se obtienen estas
probabilidades, es por eso que el foco de esta sección es indagar sobre los
modelos probabilísticos.

En estos modelos se busca modelar la densidad condicional $p(c_k | \vect{x})$
que representa la probabilidad de la clase $c_k$ dado $\vect{x}$. El
inconveniente con esto es que cada condicionamiento define una densidad
probabilística. Por lo que se tendría una por cada posible ejemplar $\vect{x}$
resultando intratable de modelar. No obstante, si se considera el
condicionamiento $p(\vect{x} | c_k)$ solo es necesario modelar una cantidad
pequeña de clases, en especial para un clasificador binario. Para nuestra suerte
$p(c_k | \vect{x})$, está relacionado con $p(\vect{x} | c_k)$ por la \emph{regla
de Bayes}.

\begin{align}
    p\left( c_1 | \vect{x} \right) &= \frac{p\left( \vect{x} | c_1 \right)p(c_1)}{
                                            p\left( \vect{x} | c_1 \right) p(c_1) + 
                                            p\left( \vect{x} | c_2 \right)p(c_2)} \\
                                   &= \frac{1}{1 + \exp\left( -a \right)} \\
                                   &= \sigma(a)
\end{align}

Donde definimos $a = \ln \frac{p\left( \vect{x} | c_1 \right) p\left( c_1
\right)}{p\left( \vect{x} | c_2 \right) p\left( c_2 \right)}$. La función
$\sigma(a)$ es conocida como la función \emph{sigmoide} y juega un importante
rol en muchos algoritmos de clasificación por su propiedad de simetría
$\sigma(-a) = 1 - \sigma(a)$, el significado de su inversa $a = \ln \left(
\frac{\sigma}{1 - \sigma} \right)$ que representa el logaritmo del radio de las
probabilidades de las clases $c_1$ y $c_2$, y la capacidad de mapear el eje real
en un intervalo finito.

Se puede generalizar esta ecuación para el caso $k > 2$ de la siguiente manera:

\begin{align}
    p\left( c_k | \vect{x} \right) &= \frac{p\left( \vect{x} | c_k \right)p(c_k)}{
                                                \sum_{j=1}^{k} p\left( \vect{x} |
                                                c_j \right) p(c_j) } \\
                                       &= \frac{\exp\left( a_k \right)}{\sum_{j=1}^{k} a_j} \\
\end{align}

Donde $a_k = \ln p\left( \vect{x} | c_k \right) p\left( c_k \right)$. La función
$\frac{\exp\left( a_k \right)}{\sum_{j=1}^{k} a_j}$ es conocida como la función
\emph{softmax} ya que es una versión suavizada de la función $max$ en el sentido
de si $a_k >> a_j$ para todo $j \neq k$ entonces $p(c_k | \vect{x}) \approx 1$ y
$p(c_j | \vect{x}) \approx 0$.

\subsection{Función de costo}

Varios de los métodos de machine learning existentes no pueden obtener los pesos
$W$ de manera analítica. Es por eso que se hacen uso de métodos iterativos en
base a una \emph{función de costo} que mide la precisión de su predicción en
comparación con la etiqueta. Si tenemos ejemplos de entrenamiento
$\{(\vect{x}_{1}, y_{1}), ..., (\vect{x}_{n}, y_{n})\}$ podemos definir la
función de costo en base a cada ejemplar $i$.

Un ejemplo clásico de función de costo es el error cuadrático medio:

\begin{align}
    J(\vect{w}) = \frac{1}{2n} \sum_{i=1}^{n}\left( h_{\vect{w}}\left( \vect{x}_{i}\right) - y_i \right)
\end{align}

Por lo tanto, al minimizar $J$ se estará reduciendo el error de estimación de
$h$ obteniendo que $h_{\vect{w}}\left( \vect{x}_{i} \right) \approx
f(\vect{x}_{i})$. Para lograr tal minimización, existe una técnica denominada
\emph{descenso por el gradiente}. la cual actualiza de manera iterativa el
vector $\vect{w}$ logrando la convergencia de la función de costo $J$ a un
mínimo valor.

\subsection{Descenso por el gradiente}

Definimos $\vect{\Delta J\left( w \right)}$ al gradiente de $J$ en $\vect{w}$
como $\vect{\Delta J\left( \vect{w} \right)} = \left(\frac{\partial
J(\vect{w})}{\partial w_1}, ..., \frac{\partial J(\vect{w})}{\partial w_n}
\right)$, el vector de derivadas parciales. Este algoritmo empieza con una
inicialización aleatoria de $\vect{w}$ y calcula $\vect{\Delta J\left( w
\right)}$ por una cantidad de iteraciones. El negativo del gradiente indica la
dirección de mayor descenso en el cual la función $J(\vect{w})$ disminuye. Por
lo tanto si al vector $\vect{w}$ se le suma el negativo del gradiente se estaría
dirigiéndolo en torno a un mínimo local de la función de costo. La magnitud del
desplazamiento vectorial en cada iteración está determinado por una constante
$\alpha$ conocida como la \emph{taza de aprendizaje}.

\begin{align} \label{eq:gradient_descent}
    \vect{w} \leftarrow \vect{w} - \alpha \vect{\Delta J\left( w \right)}
\end{align}

\begin{figure}
    \centering
    \includegraphics[scale=0.5]{figures/gradient_descent_plot.png}
    \caption{Intuición del método de descenso por el gradiente a partir del error cuadrático medio como función de costo $J$}
    \label{fig:gradient_descent}
\end{figure}

En cada iteración se realiza la operación de la ecuación actualizando
$\vect{w}$. Dado a que $\alpha$ > 0, el valor de cada parámetro en $\vect{w}$
disminuirá y eventualmente la función de costo $J$ convergerá a su valor mínimo
en $T$ iteraciones. Algo a notar es que en la ecuación \ref{eq:gradient_descent},
al ser una operación vectorial, cada componente se actualiza simultáneamente. La
figura \ref{fig:gradient_descent} muestra la intuición detrás de este método
utilizando el error cuadrático medio como función de costo.

El método de descenso por el gradiente es también llamado el optimizador de la
función de costo. En la literatura hay muchas variantes de este método que
llevan a optimizadores más complejos y eficientes, tales como \emph{descenso por
el gradiente estocástico}\citep{Kiefer-1952}, \emph{Adam} \citep{kingma-adam-2017}, \emph{NAdam}
\citep{zhang-2018}, entre otros.

\subsection{Sobreajuste (overfitting)}

Una vez optimizado la función de costo y haber encontrado la configuración de
pesos $\vect{w}$, se pueden utilizar estos pesos aprendidos para hacer
predicciones de ejemplos no antes vistos en el proceso de entrenamiento. Sin
embargo, algo que suele ocurrir al utilizar solamente la función de costo para
optimizar modelos de aprendizaje automático, es que tienden a sobreajustar los
datos de entrenamiento reduciendo su error pero obteniendo malas predicciones en
estos nuevos ejemplares. Por lo general, suele asociarse con la magnitud de los
coeficientes en $\vect{w}$ siendo muy alta para estos casos. Otras razones de
esto suele ser la dimensión de $\vect{w}$. En cuanto mayor la cantidad de
parámetros de ajuste, existen más chances de que el modelo de aprendizaje
automático memorice los datos de entrenamiento. Para solucionar esto se suele
recurrir a varias técnicas con el fin de obtener modelos sencillos pero que
logren generalizar la tarea que se está queriendo clasificar. Uno de ellos es
agregar a la función de costo un \emph{término de regularización} sobre las
componentes de $\vect{w}$ para penalizar el modelo si la magnitud $\vect{w}$
crece demasiado. Por ejemplo para el error cuadrático medio:

\begin{align}
    J(\vect{w}) = \frac{1}{2n} \sum_{i=1}^{n}\left( h_{\vect{w}}\left( \vect{x}_{i}\right) - y \right) +
                  \frac{\lambda}{2} ||\vect{w}||^{2}
\end{align}

Donde $||\vect{w}||$ es la norma de $\vect{w}$, y $\lambda$ es la fuerza en que
la regularización impacta.

Otros métodos para el control del overfitting dependen del método de
clasificación que se utilice, función de costo, parámetros de ajuste, entre
otros que iremos explicando a medida que avancemos con los modelos utilizados en
los experimentos.

\subsection{Regresión logística}

El primer modelo de clasificación que veremos es la \emph{regresión logistica}.
Es un clasificador binario que busca modelar la probabilidad a posterior de
$c_1$ a partir de una función sigmoide actuando sobre los vectores de entrada:

\begin{align}
    p\left( c_1 | \vect{x} \right) = \sigma\left( \vect{w}^{\top} \vect{x} \right)
\end{align}

Luego $p\left( c_2 | \vect{x} \right) = 1 - p\left( c_1 | \vect{x} \right)$.
Para un espacio de características $\vect{x}$ de $D$ dimensiones este modelo
consta de $D$ parámetros para ajustar. En particular este método suele codificar
las etiquetas de las clases $c_1$ y $c_2$, como $0$ y $1$, ya que permite
algunas simplificaciones en la definición de su función de costo.

De esta manera podemos pensar la etiqueta $y$ como observaciones discretas de
una distribución Bernoulli.

\begin{align}
    & p(y = 1 | \vect{x}) = h_{\vect{w}}(\vect{x}) \\
    & p(y = 0 | \vect{x}) = 1 - h_{\vect{w}}(\vect{x}) \\
    & p(y | \vect{x}) = (h_{\vect{w}}(\vect{x}))^{y}(1 - h_{\vect{w}}(\vect{x}))^{1 - y}
\end{align}

Luego los pesos $\vect{w}$ se obtienen a partir de un optimizador del tipo del
descenso por el gradiente minimizando la función de costo definida como:

\begin{align}
    J\left( \vect{w} \right) &= \sum_{i=1}^{n} p(y_{i} | \vect{x}_{i}) \\
                             &= -\sum_{i=1}^{n}
                                    y_{i}\log\left( h_{\vect{w}}\left( \vect{x}_{i} \right) \right) +
                                    (1 - y_{i})\log\left(1 - h_{\vect{w}}\left( \vect{x}_{i} \right) \right)
\end{align}

Algo a notar es que la función de costo se puede expresar como suma de
penalizaciones individuales:

\begin{align}
    J\left( \vect{w} \right) &= \sum_{i=1}^{n} \ell\left( h_{\vect{w}}\left( \vect{x}_{i}\right), y_{i}\right)
\end{align}

Donde $\ell$ es la penalización a nivel de ejemplo:

\begin{align}
    \ell\left( \vect{x}, y \right) =
    \begin{cases}
        - \log\left( h_{\vect{w}}\left( \vect{x} \right) \right) & y = 1 \\
        - \log\left(1 - h_{\vect{w}}\left( \vect{x} \right) \right) & y = 0
    \end{cases}
\end{align}

\begin{figure}
    \centering
    \includegraphics[scale=0.5]{figures/logistic_regression_loss.png}
    \caption{Gráfica de $h_{\vect{w}}(\vect{x})$ vs $\ell(\vect{x})$ separando
    por etiqueta}
    \label{fig:lgr_loss}
\end{figure}

Si analizamos la forma de $\ell$ para ambos casos podemos observar que, $J$
captura la intuición de que mayores errores en la predicción deben recibir
mayores penalizaciones.

Para el caso $y = 1$. La función de costo a nivel de ejemplo es la curva que se
marca en color azul de la figura \ref{fig:lgr_loss}. En ella se puede ver que si
$h_{\vect{w}}\left( \vect{x} \right) \rightarrow 0$ (la función de predicción
asigna a $\vect{x}$ una probabilidad cercana a $0$), entonces el costo
incrementa y por lo tanto $\ell(\vect{x}, y) \rightarrow \infty$. Si
$h_{\vect{x}}\left( \vect{x} \right) \rightarrow 1$, entonces el costo disminuye
y $\ell(\vect{x}, y) \rightarrow 0$. De manera similar ocurre para el caso $y =
0$ y la curva marcada en rojo.

\subsection{Redes neuronales}

Una red neuronal artificial es un modelo computacional que en los últimos
tiempos se ha convertido en el estado del arte y el enfoque más utilizado en la
mayoría de las áreas relacionadas al diseño e implementación de sistemas
predictivos. Estas áreas incluyen, entre otras el procesamiento del habla, la
visión por computadoras, el procesamiento del lenguaje natural y la toma de
decisiones y control en agentes situados.

El término surge bajo de los intentos de definir un modelo matemático de una red
neuronal biológica \citep{McCulloch-Pitts-1990}. No obstante, enfocaremos
nuestra atención en un tipo especifico de red neuronal reconocida por su alto
valor práctico, denominado \emph{perceptrón multicapa}.

Las redes neuronales surgen de la necesidad de aprender la representación de los
datos cuya disposición en el espacio requieren de una modelización no lineal
mucho más compleja. Similar a un modelo lineal, las redes neuronales reciben un
vector de entrada $\vect{x}$ donde cada componente $x_i$ es ponderada mediante
una combinación lineal por el peso $w_i$ en $\vect{w}$. La única diferencia es
que al resultado se le aplica una función de transformación no lineal $g$
conocida como \emph{función de activación}.

\begin{equation} \label{eq:nn}
    z_{\vect{w}}\left( \vect{x} \right) = g\left( \sum_{i=0}^{D} w_i x_i \right)
\end{equation}

Notar además que se puede pensar está red neuronal simple como un modelo de
regresión logística si la función de activación es la sigmoide, es decir, si $g
= \sigma$.

\subsubsection{Perceptrón multicapa}
\label{subch:multi_layer_perceptron}

Este modelo puede describirse como una serie de transformaciones similares a la
ecuación \ref{eq:nn}. Primero construimos $M + 1$ combinaciones lineales a
partir de las componentes del ejemplo de entrada $\vect{x} = (x_0, ..., x_D)$.

\begin{equation}
    a_j = \sum_{i = 0}^{D} w_{ji}^{(1)} x_{i}
\end{equation}

Donde $j = 0,..., M$. El supra-índice $(1)$ indica que los parámetros
correspondientes $w_{ji}^{(1)}$ están en la primera capa de la red. Luego cada
$a_j$ es transformado por una función de activación no lineal $g$.

\begin{figure}
    \centering
    \includegraphics[scale=0.5]{figures/multilayer_perceptron.png}
    \caption{Perceptrón multicapa de una sola capa oculta}
    \label{fig:nn_multilayer_perceptron}
\end{figure}

\begin{equation}
    z_j = g(a_j)
\end{equation}

Estas cantidades corresponden a las salidas de cada neurona o unidad. Luego para
estos valores construimos nuevamente $K$ combinaciones lineales obteniendo $K$
neuronas de salida.

\begin{equation}
    a_k = \sum_{j=0}^{M} w_{kj}^{(2)}z_j
\end{equation}

Donde $k= 1,..., K$. Esta transformación corresponde con la segunda capa de la
red, como lo indica el supra-índice $(2)$. Finalmente los unidades de salidas
realizan su predicción $y_k$ activando los datos por última vez con otra función
de activación. Esta última depende de la codificación de la variable objetivo, y
si se está trabajando sobre un problema de regresión o clasificación. Para el
caso de regresión la función de activación de salida suele ser la función de
identidad, para clasificación binaria una sigmoide, y para multiclase una
softmax.

\begin{equation}
    y_{k} = \sigma\left( a_k \right)
\end{equation}

Podemos combinar estas dos transiciones y dar una expresión del modelo completo
en función de $\vect{x}$ y $\vect{w}$.

\begin{equation}
    y_k(\vect{x}, \vect{w}) = \sigma\left(
                \sum_{j=0}^{M} w_{kj}^{(2)}
                    g\left( \sum_{i = 0}^{D} w_{ji}^{(1)} x_{i}
                \right)
            \right)
\end{equation}

Por lo tanto un modelo neuronal es simplemente una función no lineal que depende
de un ejemplo $\vect{x}$, que retorna un vector de salida $\vect{y}$, y que es
controlado por un vector de parámetros $\vect{w}$, donde las dimensiones de
estos vectores son $D + 1$, $K$, y $(D + 1 \times M + 1)$ respectivamente. Esta
función también puede representarse a partir de un gráfico de red como el que se
vé en la figura \ref{fig:nn_multilayer_perceptron} . El proceso de evaluar un
ejemplo puede interpretarse como una propagación hacia adelante (\emph{forward
propagation} en inglés), que transmite la información a través de la red. Por
último, algo a notar son las componentes $w_{j0}^{(1)}$ y $w_{k0}^{(2)}$ que se
corresponden con el sesgo de los modelos lineales.

\subsubsection{Backpropagation}
\label{subch:backpropagation}

Una vez determinado explícitamente el modelo, es necesario encontrar el vector
$\vect{w}$ que ajuste a los datos de entrenamiento y sea capaz de generalizar a
ejemplares nuevos. Para ello, similar a los modelos lineales se debe definir una
función objetivo a optimizar, es decir, una función de costo en compañía de una
regularización para prevenir sobreajuste, a la cual minimizar mediante algún
método del tipo descenso por el gradiente. El inconveniente para este caso es
que la función objetivo, en particular la de costo, depende del modelo en
cuestión para comparar su predicción con la etiqueta real. Lo cual dificulta el
cálculo del gradiente y el cálculo de los parámetros en la red. Es por eso que,
el método de \emph{backpropagation}, propaga el error de la capa de salida hacia
las capas iniciales derivando el gradiente de manera iterativa y asignándole a
cada neurona una porción de error en relación a su aporte generado en la salida
original.

En resumen el algoritmo de aprendizaje de una red neuronal consiste en:

\begin{itemize}
    \item Inicializar el vector de pesos $\vect{w}$ aleatoriamente.
    \item Realizar forward propagation sobre una entrada para obtener
    predicciones.
    \item Calcular el error cometido en base a una función de optimización
    (función de perdida y regularización).
    \item Realizar backpropagation para propagar el error a los parámetros de
    cada interconexión neuronal actualizándolos mediante descenso por el
    gradiente.
    \item Repetir los pasos para cada ejemplo de entrada.
\end{itemize}

\subsubsection{Funciones de activación}

Hasta el momento no mencionamos la forma de la función de activación $g$ siendo
la protagonista para obtener un modelo no lineal. Algunas de las funciones más
comunes es la \emph{sigmoide} (figura \ref{fig:nn_sigmoid}) introducida en la
sección \ref{subch:prob_models}. Esta toma valores reales y los mapea al rango
$[0, 1]$ siendo muy utilizada en procesos de clasificación.

Otras funciones muy usadas son la \emph{tanh} (tangente hiperbólica, figura
\ref{fig:nn_tanh}) y \emph{ReLU} (unidad rectificadora lineal, figura
\ref{fig:nn_relu}).

\begin{equation}
    \tanh\left( x \right) = \frac{2}{1 + \exp(-2x)} - 1
\end{equation}

\begin{equation}
    ReLU\left( x \right) = \max(0, x)
\end{equation}

\begin{figure}
    \centering
    \begin{minipage}[b]{0.4\textwidth}
      \includegraphics[scale=0.5]{figures/relu.png}
      \caption{Función de activación $ReLU$.}
      \label{fig:nn_relu}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.4\textwidth}
      \includegraphics[scale=0.5]{figures/tanh.png}
      \caption{Función de activación $tanh$.}
      \label{fig:nn_tanh}
    \end{minipage}
    \begin{minipage}[b]{0.4\textwidth}
      \includegraphics[scale=0.5]{figures/sigmoid.png}
      \caption{Función de activación $\sigma$.}
      \label{fig:nn_sigmoid}
    \end{minipage}
\end{figure}

Notar que la tangente hiperbólica realiza un cambio de escala de la sigmoide
mapeando valores reales al rango $[-1, 1]$ como se muestra en la ecuación
\ref{eq:tanh_vs_sigmoid}.

\begin{equation} \label{eq:tanh_vs_sigmoid}
    \tanh\left( x \right) = 2 \sigma(2x) - 1
\end{equation}

En el caso de la \emph{ReLU} cumple el rol de quitar los valores negativos
dejando invariantes los positivos.

\subsubsection{Variantes de arquitectura y métodos de entrenamiento}

El modelo y procedimiento definido en las secciones
\ref{subch:multi_layer_perceptron}, \ref{subch:backpropagation}, conforman una
de las variantes más simples de una red neuronal multicapa. En particular
existen formas de alterar este proceso que permiten dar cierta flexibilidad al
predictor en algunas circunstancias. Una de ellas es la posibilidad de
``apagar`` neuronas aleatoriamente con el fin de evitar el sobreajuste. Por lo
general a mayor cantidad de capas y neuronas, hay más posibilidad de
sobreajustar el conjunto de entrenamiento. Es por eso que este método,
denominado \emph{dropout}, puede reducir la complejidad del modelo dinámicamente
durante el proceso de entrenamiento.

Otra variante es el uso de \emph{batches} al actualizar $\vect{w}$ recopilando
el error de varios ejemplares para su posterior propagación a diferencia de
procesar a nivel de ejemplos.

También es posible agregar normalización de características en las capas
iniciales de la red neuronal. El uso de escalas distintas en las componentes de
los vectores de entrada puede llevar a ponderar el cálculo del error cometido en
una dimensión por sobre otra.

\subsection{XGBoost}
\label{alg:xgboost}

El ultimo modelo de aprendizaje que veremos es \emph{XGBoost} (eXtreme Gradient
Boosting en inglés) \citep{Chen-2016}, un modelo de \emph{ensemble} que consiste
en combinar la respuesta de varios modelos para obtener una final. Es decir, en
lugar de consultar la predicción de un modelo, $K$ de ellos son utilizados. Este
tipo de heurísticas y en particular la de XGBoost han sido enormemente
utilizadas por científicos de datos siendo el estado del arte en múltiples
problemas de aprendizaje automático. Desde obtener un gran desempeño en
\emph{benchmarks}, hasta lograr los primeros puestos en competencias de
aprendizaje automático como Netflix prize, Kaggle, y KDDCup.

Otro de los aspectos importantes detrás del éxito de XGBoost es su escalabilidad
en todo tipo de escenarios, siendo muy rápido para manejar consultas de billones
de ejemplos en dominios donde se cuenta con una cantidad limitada de recursos.
Esto motivó aún más su uso en esta tesis, principalmente por la necesidad de
modelos con un rápido tiempo de respuesta durante el proceso de grounding
heurístico.

A diferencia de la regresión logística y las redes neuronales, XGBoost se basa
en una estructura completamente distinta para la definición de sus modelos.
Estas consisten de árboles de decisión donde cada uno aporta su predicción a un
ejemplo de entrada. El objetivo de utilizar varios clasificadores es con la
iniciativa de que se complementen entre sí. Por ejemplo, supongamos que queremos
averiguar si una persona le gusta un videojuego de computadora. Los datos de
entrada serían vectores cuyas componentes representen su edad y la frecuencia en
la que utilizan el ordenador. Si mantenemos dos árboles para cada una de estas
componentes podemos complementar sus respuestas (figura \ref{fig:xgboost_tree})
en lugar de tener solo uno (figura \ref{fig:xgboost_two_tree}).

\begin{figure}
    \centering
    \includegraphics[scale=0.4]{figures/xgboost_tree.png}
    \caption{Árbol que decide por edad de las personas si le interesa un
    videojuego de computadora. Imagen adaptada de \citep{Tianqi-2016}}
    \label{fig:xgboost_tree}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[scale=0.4]{figures/xgboost_two_tree.png}
    \caption{Dos arboles que en base a la edad y uso de la computadora deciden
    si una persona le interesa un videojuego. Imagen adaptada de
    \citep{Tianqi-2016}}
    \label{fig:xgboost_two_tree}
\end{figure}

Nuevamente consideremos un conjunto de datos etiquetados $\{(\vect{x}_1, y_1),
..., (\vect{x}_n, y_n)\}$ donde cada $\vect{x}_i \in \mathbb{R}^{D}$. Un
modelo de \emph{ensemble} del tipo \emph{boosting tree} puede escribirse
matemáticamente como una suma de $K$ funciones que dependan de un ejemplo
$\vect{x}_i$.

\begin{equation} \label{eq:xgb-model-1}
    \hat{y_{i}} = \sum_{k=1}^{K} f_{k} \left( \vect{x}_{i}\right)
\end{equation}

Donde cada $f_k \in \mathcal{F}$, siendo $\mathcal{F}$ la familia o espacio de
todos los árboles. Ahora bien, ¿Como están definido cada uno de los árboles de
decisión $f_k$?¿De qué forma son los elementos en $\mathcal{F}$? ¿Cuales son los
parámetros del modelo?.

Debemos representar matemáticamente la estructura del árbol que asigne un
ejemplo de entrada $\vect{x}$ a una correspondiente hoja y por ende a un puntaje
como vimos en las figuras \ref{fig:xgboost_tree}, y \ref{fig:xgboost_two_tree}.
Los parámetros a encontrar son tales puntajes y se suele referir a ellos como
los pesos del modelo. Otro factor importante es la manera en que se determina la
estructura que debe tener cada árbol.

Formalmente $\mathcal{F} = \{f(\vect{x}) = w_{q\left( \vect{x} \right)}\}$ donde
$q : \mathbb{R}^{D} \rightarrow T$ y $\vect{w} \in \mathbb{R}^{T}$. Aquí $q$
representa el mapeo de un ejemplo $\vect{x}$ a una hoja en el árbol. $T$ es el
número de hojas de este último. Y $w_{q\left( \vect{x} \right)}$ es una
componente de $\vect{w}$ que representa el puntaje asociado al ejemplo
$\vect{x}$ según la estructura del árbol determinada por $q$.

Luego como los modelos vistos hasta ahora, para determinar los pesos $\vect{w}$
y la estructura del árbol es necesario definir una función objetivo a minimizar
compuesta por una función de costo y su regularización. No obstante, aprender la
estructura es mucho más complejo que sólo utilizar algún método del tipo
descenso por el gradiente. Es por eso que la estrategia utilizada por los
algoritmos de \emph{boosting} es agregar de manera iterativa un nuevo árbol a lo
aprendido, en particular, aquellos árboles que optimicen algún objetivo. Esto da
lugar a definir la predicción del modelo al paso $(t)$.

\begin{align*}
    \hat{y_i}^{(0)} &= 0 \\
    \hat{y_i}^{(1)} &= f_{1}\left( \vect{x}_i \right) = \hat{y_i}^{(0)} + f_1{(\vect{x}_i)} \\
    ... \\
    \hat{y_i}^{(t)} &= \sum_{k=1}^{t} f_{k} \left( \vect{x}_{i}\right) = \hat{y_i}^{(t - 1)} + f_t{(\vect{x}_i)}
\end{align*}

Luego, la función a optimizar en la iteración $(t)$ se define como:

\begin{align}
    \mathcal{L}^{(t)} &= \sum_{i=1}^{n} \ell\left( y_i, \hat{y_i}^{(t)} \right) + \Omega(f_t) \\
                      &= \sum_{i=1}^{n} \ell\left( y_i, \hat{y_i}^{(t - 1)} + f_t{(\vect{x}_i)} \right) + \gamma T + \frac{1}{2} \lambda \sum_{j=1}^{T} w_{j}^{2}
\end{align}

Con $\Omega(f_t)$ la regularización para el predictor. $\gamma$ y $\lambda$ son
parámetros de $\Omega$ que intensifican o disminuyen la regularización.

El último requerimiento es optimizar $\mathcal{L}$ para un paso arbitrario y el
significado de la función de costo $\ell$. XGBoost admite cualquier función de
perdida que aproxima a partir de una expansión de Taylor de segundo orden.

\begin{equation} \label{eq:xgboost-loss-taylor}
    \mathcal{L}^{(t)} = \sum_{i = 1}^{n} [
        \ell\left(y_i,
                  \hat{y_{i}}^{(t-1)}
            \right) +
                g_i f_t(\vect{x}_i) +
                \frac{1}{2} h_i f_t^{2}(\vect{x}_i)
        ] + \gamma T + \frac{1}{2} \lambda \sum_{j=1}^{T} w_{j}^{2}
\end{equation}

Donde $g_i$ y $h_i$ son definidas como:

\begin{align}
    g_i &= \partial_{\hat{y_{i}}^{(t-1)}} \ell\left(y_i, \hat{y_{i}}^{(t-1)}\right) \\
    h_i &= \partial_{\hat{y_{i}}^{(t-1)}}^{2} \ell\left(y_i, \hat{y_{i}}^{(t-1)}\right)
\end{align}

Algo a notar de la ecuación \ref{eq:xgboost-loss-taylor} es que $\sum_{i =
1}^{n} \ell\left(y_i, \hat{y_{i}}^{(t-1)} \right)$ es una suma realizada hasta
un paso anterior, estando ya calculado por el algoritmo y por lo tanto se
considera constante para el paso actual $(t)$. Omitiendo dicho término se
obtiene:

\begin{align}
    \mathcal{L}^{(t)} &= \sum_{i = 1}^{n} [
        g_i f_t(\vect{x}_i) +
                \frac{1}{2} h_i f_t^{2}(\vect{x}_i)
        ] + \gamma T + \frac{1}{2} \lambda \sum_{j=1}^{T} w_{j}^{2} \\
                      &= \sum_{i = 1}^{n} [ \label{eq:xgboost-loss-final}
                        g_i w_{q(\vect{x}_i)} +
                                \frac{1}{2} h_i w_{q(\vect{x}_i)}^{2}
                        ] + \gamma T + \frac{1}{2} \lambda \sum_{j=1}^{T} w_{j}^{2}
\end{align}

Se puede identificar la penalización de la función de costo que se agrega al
peso $w_j$ como $G_{j} = \sum_{i \in I_j} g_i$ y $H_j = \sum_{i \in I_j} h_i$
con $I_j = \{i | q(\vect{x}_i) = j\}$ (Conjunto de índices de ejemplares que son
asignados a la $j$-ésima hoja). Es decir, cada componente $w_j$ es penalizada
$|I_{j}|$ veces por $g_i$ y por $h_i$. Luego la ecuación
\ref{eq:xgboost-loss-final} se puede escribir como:

\begin{equation}
    \mathcal{L}^{(t)} = \sum_{j = 1}^{T}[G_j w_j + \frac{1}{2} (H_j + \lambda)w_j^2] + \gamma T
\end{equation}

Finalmente, igualando a $0$ y despejando $w_j$ se obtienen los $w_j$ óptimos
para un árbol con estructura $q(\vect{x})$ y la mejor forma de evaluar su
performance son los de las ecuaciones \ref{eq:xgboost-best-w} y
\ref{eq:xgboost-best-opt}.

\begin{align}
    w_{j}^{*} &= - \frac{G_j}{H_j + \lambda} \label{eq:xgboost-best-w}\\
    \mathcal{L}^{*} &= -\frac{1}{2} \sum_{j=1}^{T} \frac{G_{j}^{2}}{H_j + \lambda} + \gamma T \label{eq:xgboost-best-opt}
\end{align}

En resumen, para una estructura de árbol dada, cada ejemplo es evaluado
asignándole una hoja. Se recopilan los ejemplos que pertenecen a cada una de
ellas y se penaliza según los estadisticos $g_i$ y $h_i$ y finalmente obtener el
error cometido por el árbol $\mathcal{L^{*}}$. Aquel árbol que tenga menor error
es el mejor para ser incluido en la iteración actual.

\section{Codificación de características}

Una vez revisado los modelos de aprendizaje, otro factor importante para obtener
un buen modelo de aprendizaje automático es la representación de los datos de
entrada. Preparar los ejemplares para acoplarse apropiadamente a un algoritmo
con el fin de mejorar la performance del modelo es una de las tareas que
los científicos de datos disponen la mayor parte de su atención y tiempo. Alguna
de las obligaciones que incluye son la imputación de valores faltantes, manejo
de valores atípicos, estandarización y escalado, transformación de
características numéricas a categóricas, y codificación.

En particular pondremos nuestro foco en la codificación de características
desarrollando métodos del tipo \emph{one-hot} y \emph{word embeddings}.

\subsection{One-hot encoding}
\label{method:ohe}

A menudo los datos disponibles presentan características que no están dadas en
un espacio continuo si no más bien categórico. Por ejemplo, se podría describir
el continente de un país por las clases \emph{enAsia}, \emph{enAfrica},
\emph{enEurope}, \emph{enAmerica}, \emph{enOceanía}, \emph{enAntártida} siendo
eficientemente representables con enteros $[0, 1, 2, 3, 4, 5]$. De esta manera,
si tenemos un variable que puede tomar una serie de valores categóricos,
entonces se puede enumerar cada uno de sus objetos. Esta representación es
conocida como \emph{ordinal encoder}.

\begin{equation*}
    \begin{bmatrix}
         & enContinente\\
        Argentina & 3 \\
        Brasil & 3 \\
        Espa\tilde{n}a & 2 \\
        USA & 3  \\
        Italia & 2 
    \end{bmatrix}
\end{equation*}

Otra opción es usar un esquema \emph{one-of-K} que transforma una característica
categórica con $N$ clases, en $N$ categorias binarias con una de ellas 1 y el
resto 0. Para el ejemplo anterior tendríamos:

La misma técnica es válida si se quisiera representar oraciones o documentos del
lenguaje natural. Supongamos que tenemos los documentos.

\begin{center}
    $D_1$: ``El sol es una estrella, no es un planeta.`` \\
    $D_2$: ``La tierra es un planeta.``    
\end{center}

Basado en  estos dos textos, un vocabulario de 10 palabras distintas es
construido. Por lo tanto cada documento es representado como un vector de 10
dimensiones.

\begin{table}[H]
    \centering
    \scalebox{0.9}{
     \begin{tabular}{||c | c | c | c | c | c | c | c | c | c | c ||} 
     \hline
      -  & El & sol & es & una & estrella & no & un & planeta & la & tierra \\ [0.5ex] \hline\hline
      $D_1$ & 1 & 1 & 2 & 1 & 1 & 1 & 1 & 1 & 0 & 0  \\
      $D_2$ & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 1 & 1 \\ [1ex]
      \hline
     \end{tabular}}
\end{table}

Observar que para este caso una codificación ordinal no sería adecuada dado que
se deberían enumerar todas las palabras del vocabulario.

\subsection{Vectores de palabras (Word embeddings)}
\label{method:wb}

Un vector denso de palabras (comúnmente conocido como word embedding, en inglés)
es una de las técnicas de representación de vocabulario más popular en el área
del procesamiento de lenguaje natural.

Si bien hay distintas formas de representar palabras en una serie de documentos,
los word embeddings proveen otra perspectiva que busca encontrar una
representación vectorial compacta donde cada dimensión logre capturar las
propiedades subyacentes y latentes de la palabra. De esta manera, los embeddings
son superiores a una representación que permanece en un nivel poco profundo.

Su principal característica  es que las expresiones relacionadas entre sı́ (ya
sea por contexto, semántica o sintaxis) se encuentren cercanas en el espacio
vectorial al cual se proyectan. Por ejemplo, si tenemos las oraciones: ``Que
tengas una buena mañana`` y ``Que tengas una buena tarde``. Difieren
semánticamente solo por las palabras ``mañana`` y ``tarde``. No obstante, se
utilizan en contextos similares, con lo cual esperaríamos que estén cercanas
vectorialmente.

Otro ejemplo muy famoso que deja en evidencia el concepto de \emph{analogía} en
word embeddings, es el que se muestra en la Figura \ref{fig:king-queen-example}.
En ella se identifican por medio de barras de colores la dimensión de los
vectores asociados a las palabras \emph{king}, \emph{man}, \emph{woman},
\emph{queen}, y la operación vectorial \emph{king - man + woman}. Cada dimensión
se encuentra en la escala de $[-2, 2]$ y se le asigna el color de rojo para
valores cercanos al extremo derecho, azul para las del extremo izquierdo, y
blanco para las que se encuentren alrededor del 0.  Algo a notar es que
\emph{man} y \emph{woman} son mucho más similares de lo que cada uno es con
\emph{king} o \emph{queen}. Otro aspecto interesante son las franjas roja y azúl
que se muestran para cada uno de los ejemplos, indicando que son parecidos en
tal dimensión. Probablemente corresponda a alguna característica humana aunque
se desconoce que puedan llegar a significar debido a la poca interpretabilidad
del espacio al cual se proyectan.

Aun así, lo importante es la comparación entre palabras. Dado que ahora tienen
una representación vectorial, se pueden sumar o restar obteniendo nuevos
resultados que forman parte del espacio. Este es el caso del vector dado por
\emph{king - man + woman}. No conocemos que dimensiones exactamente capturan que
rey es utilizado para identificar al monarca de un reino, pero se puede
substraer el caracter masculino del vector \emph{man} y agregar las de
\emph{woman} esperando que se aproximen a las de \emph{queen}.
Sorprendentemente, de un total de 400000 palabras sobre las cuales se realizó el
experimento, reina fue la palabra más cercana. Los detalles del mismo se pueden
ver desde \citep{jalammar-website}.

\begin{figure}
    \centering
    \includegraphics[scale=0.68]{figures/king-analogy-viz.png}
    \caption{Vectores de palabras para ``king``, ``man``, y ``woman``}
    \label{fig:king-queen-example}
\end{figure}

Esto refleja el gran potencial que tienen los word embeddings y su importancia
para estudiarlos como métodos de codificación. A continuación analizaremos como
se calculan dichas representaciones y que algoritmos fueron finalmente
utilizados en esta tesis.

\subsubsection{Modelos de lenguaje}

Después de ver el potencial de los embeddings, surge la pregunta, ¿Cómo
funcionan internamente? ¿De qué manera se obtienen estos vectores de palabras?
En \citep{firth-57} se mostró que el significado de una palabra está determinado
según sus palabras vecinas. Si contemplamos la oración ``Que tengas un buen`` y
se quisiese averiguar cual es la expresión que le sigue, lo más probable es que
las palabras días, tarde, noche, o semana, hayan sido las primeras candidatas.
Este tipo de tareas en las que se intenta predecir información de la oración a
partir del contexto o viceversa son denominadas tareas de pretexto y son
utilizadas para entrenar un clasificador que resuelva esta tarea, únicamente
para preservar su representación a nivel de palabras.

\subsubsection{Skipgram}

Es un método desarrollado en \citep{Mikolov-2013} cuyo objetivo es, para un
vocabulario de tamaño $W$ donde una palabra es identificada por su índice $w \in
\{1, ... W\}$, aprender una representación vectorial para cada $w$. Estas
representaciones son entrenadas para predecir palabras vecinas a partir de una
actual. Es decir, dado un largo corpus de entrenamiento, considerarlo como una
sola secuencia de palabras $w_1, ... w_T$ y maximizar la expresión en
\ref{eq:skipgram-ideal}

\begin{equation} \label{eq:skipgram-ideal}
    \frac{1}{T} \sum_{n=1}^{T}
                    \sum_{c \in C_t} \log P(w_c | w_t)
\end{equation}

Donde $C_t$ es el conjunto de indices de palabras que rodean a $w_t$. La
probabilidad de observar una palabra del contexto $w_c$ dado $w_t$ es modelada a
partir de sus vectores representación. Por el momento, abstraeremos esto a
partir de una función \emph{score} $s$ que mapea pares (palabra, contexto) a un
puntaje en $\mathcal{R}$. Una forma posible de definir la probabilidad de una
palabra en el contexto es por medio de la función \emph{softmax}.

\begin{equation} \label{eq:skipram-softmax}
    P(w_{c}|w_{t}) = \frac{e^{s(w_t, w_c)}}{\sum_{j=1}^{W} e^{s(w_t, j)}}
\end{equation}

No obstante, este calculo es bastante costoso computacionalmente siendo que al
final solo es de interés predecir una palabra del contexto $w_c$ dado una
palabra $w_t$. Es por eso que el problema puede ser visto como una tarea de
clasificación binaria en lugar de una multiclase. El objetivo es predecir
independientemente la presencia o ausencia de palabras de contexto. Para una
palabra en la posición $t$, consideramos todos los contextos como ejemplos
positivos, y palabras seleccionadas aleatoriamente del vocabulario, como
negativos. Para un contexto $c$, utilizando la función de costo \emph{binary
logistic}, obteniendo la siguiente función de probabilidad:

\begin{equation}
    \log\left( 1 + e^{-s(w_t, w_c)} \right) +
    \sum_{n \in \mathcal{N}_{t, c}} \log\left( 1 + e^{s(w_t, n)} \right)
\end{equation}

Donde $\mathcal{N}_{t, c}$ es una muestra aleatoria de ejemplares negativos
tomadas del vocabulario. Si denotamos la función logística como $\ell: x \mapsto
\log \left(1 + e^{-x} \right)$, podemos reescribir la función a optimizar del
problema como:

\begin{equation}
    \sum_{n=1}^{T} 
        \sum_{c \in C_t} \ell\left(s(w_t, w_c)\right) +
        \sum_{n \in \mathcal{N}_{t, c}} \ell\left(-s(w_t, n) \right)
\end{equation}

Una parametrización natural para $s$ entre una palabra $w_t$ y un contexto $w_c$
es a partir del uso de vectores de palabras. Definimos dos vectores $\vect{u}_w$
y $\vect{v}_w$ en $\mathbb{R}^D$ también conocidos como vectores \emph{input} y
\emph{output}. Es decir, tenemos vectores $\vect{u}_{w_t}$ y $\vect{v}_{w_c}$
asociados a palabras $w_t$ y $w_c$. Luego el \emph{score} puede computarse como
$s\left( w_t, w_c \right) = \vect{u}_{w_t}^\top \vect{v}_{w_c}$.

Para ejemplificar como es el proceso de entrenamiento de este modelo, tomemos la
secuencia ``Me gustaría dos rebanadas de pizza mozzarella``. Podemos pensar un
contexto como una ventana que se desliza a través del conjunto de entrenamiento.
La figura \ref{fig:wb_slices} muestra las 3 posibles ventanas de tamaño 5 que se
pueden obtener a partir de la secuencia deslizándose con un paso de 1.

Con cada ventana, seleccionamos aquella palabra que cargue más valor
informativo, que por lo general suele ser la del medio, y la utilizamos para
predecir sus vecinas. De esta manera, generamos un conjunto de entrenamiento
compuesto por pares (palabra, palabra de contexto) donde etiquetamos con 1, si
ambas son vecinas, y 0 en caso contrario. El resultado final para cada contexto
se puede ver en la figura \ref{fig:wb_all_ones_matrix}. Esta es la
simplificación del problema para trabajar sobre una tarea de clasificación
binaria. En lugar de ser multiclase cuya  etiqueta sería la palabra de contexto,
la incluimos como característica y reemplazamos las anotaciones por 1's y 0's.
Ahora bien, con nuestros datos actuales solo tendríamos etiquetas con 1's. En
tal caso, el modelo en lugar de aprender lo que necesitamos, estaría sesgado a
siempre predecir afirmativamente. Es por eso que agregar ejemplos negativos de
manera aleatoria (figura \ref{fig:wb_neg_sampling_matrix}) a cada $w_t$ resulta
una buena idea.

\begin{figure}
    \centering
    \includegraphics[scale=0.60]{figures/context_example_1.png}
    \caption{Todas las ventanas de contexto con tamaño 5 que se pueden formar
             con la frase ``Me gustaría dos rebanadas de pizza mozzarella``.}
    \label{fig:wb_slices}
\end{figure}

\begin{figure}
    \centering
    \begin{minipage}[b]{0.4\textwidth}
      \includegraphics[scale=0.5]{figures/context_example_2.png}
      \caption{Material de entrenamiento obtenido de las ventanas.}
      \label{fig:wb_all_ones_matrix}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.4\textwidth}
      \includegraphics[scale=0.5]{figures/context_example_3.png}
      \caption{Selección aleatoria de 2 ejemplos negativos por cada positivo.}
      \label{fig:wb_neg_sampling_matrix}
    \end{minipage}
\end{figure}

Luego, al inicio del proceso de entrenamiento se crean dos matrices con valores
aleatorios, una para los embeddings y otra para los contextos. Estas dos
matrices de tamaño $V \times D$, tienen una fila por cada palabra en el
vocabulario. En este caso, $D$ es la cantidad de dimensiones que tendrán sus
representaciones. En cada paso de entrenamiento, se selecciona un ejemplo
positivo y sus correspondientes ejemplos negativos (figura
\ref{fig:wb_emb_con_matrix}). Este grupo es evaluado por el modelo a partir de
los vectores representación \emph{input} y \emph{output} según el producto
escalar entre ellas y su conversión en probabilidades por medio de la función
sigmoide. A partir de la etiqueta real, se puede determinar el puntaje de error
para cada ejemplo y finalmente actualizar los parámetros de las matrices de
embeddings y contexto. Eso concluye un paso de entrenamiento, que se repite por
cada grupo de ejemplos positivos y negativos por una cierta cantidad de épocas.
Una vez concluido el proceso de entrenamiento, la matriz de embeddings contiene
las representaciones finales.

\begin{figure}
    \centering
    \includegraphics[scale=0.5]{figures/context_example_4.png}
    \caption{Matrices para los embeddings y contextos.}
    \label{fig:wb_emb_con_matrix}
\end{figure}

\subsubsection{Modelos de lenguaje a nivel de subpalabra}
\label{method:wb_subwords}

El modelo de skipgrams al usar vectores representación por cada palabra se
ignora su estructura interna. Esto fue lo que inspiró el estudio de modelos a
nivel de subpalabra en \citep{bojanowski-2017}. La principal característica de
estos modelos radica en la función de score $s$. En lugar de considerar una
palabra $w$ en su totalidad, esta es dividida en $n$-gramas de caracteres. Para
delimitar inicio y fin de una palabra se agregan símbolos especiales \verb|<| y
\verb|>| respectivamente. Esto a su vez permite distinguir prefijos y sufijos de
otras secuencias de caracteres. Por último la palabra $w$ también es incluida
como un $n$-grama o secuencia especial. Por ejemplo, la palabra \emph{color} y
$n=4$ estaría representada por los $4$-gramas:

\begin{center}
    \verb|<col|, \verb|colo|, \verb|olor|, \verb|lor>|
\end{center}

Junto a la secuencia completa \verb|<color>|. Notar que la secuencia
\verb|<olor>| correspondiente a la palabra \emph{olor} es diferente al $4$-grama
\verb|olor|. Luego lo único que queda restante es modificar la función
\emph{score} $s$ del método skipgram por la ecuación
\ref{eq:wb_subwords_s_score}.

\begin{equation} \label{eq:wb_subwords_s_score}
    s\left(w, c\right) = \sum_{g \in \mathcal{G}_w} \vect{z}_g^{\top}\vect{v}_c
\end{equation}

Donde, $\mathcal{G}_w \subset \{1, .. G\}$ es el conjunto de $n$-gramas de la
palabra $w$, $\vect{z}_g$ el vector representación del $n$-grama $g$, y $G$ la
cantidad de $n$-gramas en total.

Este modelo permite compartir representaciones entre palabras logrando aprender
una confiable codificación a palabras poco comunes siendo útil para los
experimentos de esta tesis en la tarea de codificación de planes relajados.

\subsubsection{Representación vectorial de oraciones}
\label{lit:sentence_vector}

Como cierre de esta sección presentaremos algunos métodos sencillos de
codificación de oraciones a partir de los embeddings orientadas a tareas de
clasificación que involucren manipulación de texto.

\textbf{Concatenación}. Este método consiste en concatenar los vectores de las
palabras que conforman la oración a codificar obteniendo un vector de largo
igual a la suma de las dimensiones de los vectores individuales. En este caso,
el problema evidente que surge es la representación de oraciones con distinto
largo y el tamaño del vector resultante.

\textbf{Promedio}. Como su nombre lo indica, este método computa el centroide de
los embeddings de todas las palabras que conforman una oración. Usualmente esta
codificación resulta ser adecuada siempre y cuando los vectores de las palabras
se encuentren en la misma escala, lo cual no siempre suele ser el caso. Otro
problema menos evidente es la posibilidad de ocurrencia de palabras poco
representativas pero que tengan mucho peso en la suma debido a la frecuencia en
que ocurren. Si en la oración la palabra que buscamos ocurre pocas veces puede
verse opacada.

\textbf{Promedio normalizado}. Por último, una mejora de la representación
anterior es la normalización por norma L2 de cada vector previo a realizar el
promedio.

Pueden encontrarse otras maneras de codificar los datos a través de
\citep{Iacobacci-2016} donde se explican como utilizar word embeddings para
desambiguación de sentidos, proponiendo distintos métodos de codificación de
instancias de entrenamiento a partir de un modelo de lenguaje. Esto muestra que
el uso de representaciones a nivel de palabra en la codificación de oraciones
suelen ser patrones comunes al momento de incluirlo en una arquitectura
supervisada.

\section{Métricas de clasificación}
\label{lit:metrics}

Con el fin de evaluar que tan bien se desempeña el modelo se contrastan
resultados predichos por el clasificador contra otros previamente anotados que
no formaron parte como material de entrenamiento. Para un problema de
clasificación binaria donde los positivos corresponden con la clase $1$, y los
negativos con la $0$ dispusimos de las siguientes métricas.

\textbf{Precisión}. Cociente entre verdaderos positivos y el total de
predicciones positivas. 

\begin{equation}
    precision = \frac{TP}{TP + FP}
\end{equation}

\textbf{Recall}. Cociente entre verdaderos positivos y el total de positivos.

\begin{equation}
    recall = \frac{TP}{TP + FN}
\end{equation}

\textbf{F-score} ($F_{\beta}$). El inconveniente de las primeras dos métricas es
que ambas alcanzan el $100\%$ en modelos triviales. Es decir aquellos que
predicen siempre la clase positiva obtienen un valor máximo en recall, mientras
los que predicen siempre la clase negativa consiguen un valor máximo en
precisión. La $F_{\beta}$ mide el desbalance entre las dos anteriores ponderando
una por sobre la otra de acuerdo al valor de $\beta$ pero esta vez penalizando
los modelos triviales. 

\begin{equation}
    F_{\beta} = (1 + \beta^2) \frac{precision . recall}{\beta^2 precision + recall}
\end{equation}

Valores comunes de $\beta$ suelen ser $0.5$ (priorizar precisión), $1$ (ninguna
prioridad), $1.5$ (priorizar recall).

\textbf{H-score} ($H_{\beta}$). Durante el desarrollo de la tesis se definió una
variante de la $F_{\beta}$, que denominamos $H_{\beta}$, permitiendo lidiar con
datos desbalanceados. Si la cantidad de datos negativos es mucho mayor que la de
positivos, la precisión puede verse más comprometida que la recall, al haber más
oportunidades de cometer un falso positivo. Para resolver este problema, en vez
de considerar precisión y recall en la ecuación de $F_{\beta}$, se utilizó la
\emph{taza de verdaderos positivos y negativos} (TPR, y TNR).

\begin{align}
    TPR &= \frac{TP}{TP + FN} \\
    TNR &= \frac{TN}{TN + FP} \\
    H_{\beta} &= (1 + \beta^2) \frac{TNR . TPR}{\beta^2 TNR + TPR}
\end{align}

Notar que $TNR$ continua penalizando por la cantidad de falsos positivos de
manera similar a precisión. No obstante, la métrica es relativa a la cantidad de
negativos. Si el desbalance es sobre los negativos, entonces la métrica no se ve
comprometida. De manera similar a $F_{\beta}$, un valor de $0.5$ prioriza $TNR$,
de 1 considera un balance entre ambas métricas, y $1.5$ prioriza $TPR$.

\textbf{Matrices de confusión.} En problemas de clasificación, las matrices de
confusión permiten evaluar la calidad de la clasificación comparando las
predicciones del modelo, con las etiquetas reales. Por definición, una matriz de
confusión $C$ es tal que $C_{ij}$ es igual al numero de ejemplos cuya etiqueta
es $i$, y la predicción es $j$. Por lo tanto para un problema de clasificación
binaria tenemos que:

\begin{itemize}
    \item La cantidad de verdaderos negativos (TN) es $C_{0,0}$.
    \item La cantidad de falsos negativos (FN) es $C_{1,0}$.
    \item La cantidad de verdaderos positivos (TP) es $C_{1, 1}$.
    \item La cantidad de falsos positivos (FP) es $C_{0, 1}$
\end{itemize}

La figura \ref{fig:cm_example_raw} muestra una matriz de confusión para un
problema de clasificación binaria donde las columnas representan las
predicciones, y las filas la etiqueta real. Para este ejemplo se pueden observar
60 TP, 5 FP, 187 TN, y 46 FN.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/cm_example_raw.png}
    \caption{Ejemplo de matriz de confusión en un problema de clasificación binaria.}
    \label{fig:cm_example_raw}
\end{figure}

Para los casos en que tengamos datos desbalanceados, es conveniente normalizar
la matriz de confusión. Sea una matriz de confusión $C$, definimos $CN$, la
matriz de confusión normalizada de $C$, tal que $CN_{i,j} =
\frac{C_{i,j}}{\sum_{j} C_{i, j}}$.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/cm_example_percent.png}
    \caption{Ejemplo de matriz de confusión normalizada en un problema de clasificación binaria.}
    \label{fig:cm_example_percent}
\end{figure}

Para el caso de la figura \ref{fig:cm_example_raw}, cada celda se dividió por la
cantidad de elementos totales en una fila completa obteniendo la matriz de la
\ref{fig:cm_example_percent}. De esta manera, podemos comparar los resultados en
dos categorias distintas aún si estas se encuentran desbalanceadas. En este
ejemplo observamos que un 57\% de los datos pertenecientes a la clase positiva
se clasificaron correctamente como positivos, mientras que un 97\% de las
negativas fueron clasificados como negativas.

\textbf{Gráficos de distribución de predicciones.} Por úĺtimo, la métrica
estrella de este trabajo son los gráfico de distribución de predicciones. Como
el algoritmo de grounding heurístico utiliza las probabilidades asignadas a una
acción dado su plan relajado, un histograma que refleje la distribución de las
probabilidades asociadas a cada par es excelente para determinar si las clases
positivas se distribuyen con una probabilidad mayor que las negativas.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/displot_example.png}
    \caption{Gráfico de distribución de predicciones en un problema de clasificación binaria.}
    \label{fig:distplot-example}
\end{figure}

La figura \ref{fig:distplot-example} muestra el gráfico de distribución de
predicciones para el mismo problema de clasificación binaria que utilizamos para
explicar las matrices de confusión. Las barras azules representan la clase
negativa, y las barras naranjas la clase positiva. Se busca que ambas barras
esten lo más separadas posibles. En particular, que las predicciones cuya clase
es positiva, tengan asociada una probabilidad cercana a 1, y las predicciones
cuya clase es negativa, tengan asociada una probabilidad cercana a 0.