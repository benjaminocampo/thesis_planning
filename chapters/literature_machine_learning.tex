\chapter{Aprendizaje automático}

En este capítulo se profundizará en el área de \emph{aprendizaje automático}
abarcando la representación de palabras y oraciones dadas por codificaciones del
tipo one-hot, word embeddings, conceptos de aprendizaje supervisado y no
supervisado, modelos de clasificación, ajuste de hiperparámetros, y métricas.

\section{Orígenes y evolución}

El aprendizaje automático es el campo de la inteligencia artificial que busca
desarrollar programas que aumáticamente mejoren en base a la experiencia. Estos
métodos difieren del típico paradigma de implementación donde en lugar de ser
``programado`` es ``entrenado``. Tal entrenamiento consiste en disponerle al
algoritmo información en forma de ejemplos con el fin de reconocer patrones
estadísticios y eventualmente determinar reglas que sirvan para automatizar una
tarea.

Por moderno que pueda parecer este campo, tuvo sus comienzos en los años 50 a
partir del famoso \emph{Test de Turing} que consistía de una máquina cuyo
objetivo era engañar a un humano haciéndole creer que se encontraba delante de
otra persona en lugar de un ordenador. Llegando a la conclusión de que los
computadores de propósito general, podrían ser capaces de aprender y ser de
alguna manera originales.
\footnote{\url{https://www.turing.org.uk/scrapbook/test.html}}

En los últimos años varias aplicaciones se han beneficiado de esta área, desde
programas relacionados con la detección fraudulenta de transacciones con tarjeta
de credito, sistemas de recomendación que guían usuarios en un servicio de
acuerdo a sus preferencias, o incluso vehículos que se manejan sin necesidad de
la intervención del conductor. Al mismo tiempo, una importante cantidad de
avances teóricos y algorítmicos se fueron realizando formando las bases de este
campo. TODO: Poner referencias a los ejemplos.

Aún con estos increibles logros, no se conoce aún como crear computadoras que
aprendan al nivel que las personas lo hacen. No obstante, se han desarrollado
algoritmos que se han aproximado a este objetivo siendo efectivos para varios
tipos de problemas.

\section{Aprendizaje supervisado}

El aprendizaje supervisado es una subárea del aprendizaje automático cuyo
objetivo es deducir una modelo a partir de datos anotados que permita mapear
ejemplos no vistos previamente. Esta información está compuesta por un conjunto
de ejemplares y sus correspondientes etiquetas, en ocasiones dada por un
anotador humano, la cual indica el resultado del modelo a partir del dato como
entrada. Las etiquetas pueden ser categóricas o continuas determinando un
problema de clasificación o regresión. Algunas tareas más frecuentes de
clasificación son la categorización de documentos, reconocimiento de lenguaje
ofensivo, o análisis de sentimiento. Mientras que para regresión, lo son las
estimaciones de precios de artículos, objetos, o viviendas.

El resultado de ejecutar un algoritmo de machine learning supervisado se puede
expresar como una función $f(\vect{x})$ que recibe un ejemplar $\vect{x}$ como
entrada y genera un vector $\vect{y}$ de salida codificado de la misma manera
que las etiquetas. Las muestras utilizadas para ajustar $f$ estan dadas por
vectores $\{\vect{x}_1, ..., \vect{x}_n\}$ y anotaciones $\{y_1, .. y_n\}$
conformando el \emph{conjunto de entrenamiento}.

La forma precisa de $f$ es determinada durante la fase de entrenamiento. Una vez
transcurrida, se puede estimar la identidad de nuevos datos etiquetados
pertenecientes al \emph{conjunto de test} con el fin de evaluar la performance
del modelo. En el caso que la predicción se aproxime a la esperada para estas
nuevas entradas, entonces el modelo habría logrado generalizar la tarea.

Sin embargo, los modelos de aprendizaje supervisado están limitados por datos
anotados cuya disponibilidad dependerá de la dificultad para obtenerlos. Algunas
tareas son relativamente sencillas para etiquetar para cualquier persona, (e.g.
como determinar si en una foto aparece un gato), mientras que en otros puede
llegar a requerir humanos que sean expertos de dominio (e.g. abogados, médicos,
lingüistas, etc).

\section{Aprendizaje no supervisado}

En contraste al método anterior, el aprendizaje no supervisado consiste en
descubrir automáticamente patrones sobre los datos de entrenamiento que permitan
explicarlos sin depender de datos de anotados. Es considerada como una parte del
área del análisis y exploración. Es por eso que no pueden ser evaluados
basándose en exactitud o precisión, si no más bien en la cantidad de información
que podamos extraer de los datos a partir del uso de estas técnicas. Algo a
considerar es lo sencillo que es disponer de estos datos al no requerir ningún
tipo de supervición humana.

\section{Codificación de características}

Los modelos de aprendizaje automático dependen fuertemente de como se
representen los datos de entrada que uno tiene disponible. Preparar los
ejemplares para acoplarse apropiadamente a un algoritmo de machine learning con
el fin de mejorar la performance del modelo es una de las tareas las cuales los
científicos de datos disponen la mayor parte de su atención y tiempo. Alguna de
las obligaciones que incluye son la imputación de valores faltantes, manejo de
valores atípicos, estandarización y escalado, transformación de características
numéricas a categóricas, y codificación.

En particular pondremos nuestro foco en la codificación de características
desarrollando métodos del tipo \emph{one-hot} y \emph{word embeddings (word
embeddings)}.

\subsection{One-hot encoding}

A menudo los datos disponibles presentan características que no estan dadas en
un espacio continuo si no más bien categórico. Por ejemplo, se podría describir
el continente de un país por las clases \emph{inAsia}, \emph{inAfrica},
\emph{inEurope}, \emph{inSouthAmerica}, \emph{inNorthAmerica} siendo
eficientemente representarse como enteros $[0, 1, 2, 3, 4]$. De esta manera, si
tenemos una serie de variables categóricas, estas pueden codificarse enumerando
por variable cada uno de sus objetos. Esta representación es conocida como
\emph{ordinal encoder}.

\begin{equation*}
    \begin{bmatrix}
         & inContinent\\
        Argentina & 3 \\
        Brasil & 3 \\
        España & 2 \\
        USA & 4  \\
        Italia & 2 
    \end{bmatrix}
\end{equation*}

Otra opción es usar un esquema \emph{one-of-K} que
transforma una característica categórica con $N$ clases, en $N$ categorias
binarias con una de ellas 1 y el resto 0. Para el ejemplo anterior tendríamos:

\begin{equation*}
\begin{bmatrix}
    & inAsia & inAfrica & inEurope & inSouthAmerica & inNorthAmerica \\
    Argentina & 0 & 0 & 0 & 1 & 0 \\
    Brasil & 0 & 1 & 0 & 1 & 0 \\
    España & 0 & 0 & 1 & 0 & 0 \\
    USA & 0 & 0 & 0 & 0 & 1 \\
    Italia & 0 & 0 & 1 & 0 & 0
\end{bmatrix}
\end{equation*}

La misma técnica es válida si se quisiera representar oraciones o documentos del
lenguaje natural. Supongamos que tenemos los documentos.

\begin{center}
    $D_1$: ``El sol es una estrella, no es un planeta.`` \\
    $D_2$: ``La tierra es un planeta.``    
\end{center}

Basado en  estos dos textos, un vocabulario de 9 palabras distintas es
construido. Por lo tanto cada documento es representado como un vector de 9
dimensiones.

\begin{equation*}
    \begin{bmatrix}
        & El & sol & es & una & estrella & no & un & planeta & tierra \\
        D_1 & 1 & 1 & 2 & 1 & 1 & 1 & 1 & 1 & 0  \\
        D_2 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 1 
    \end{bmatrix}
\end{equation*}

Observar que para este caso una codificación ordinal no sería adecuada dado que
se deberían enumerar todas las palabras del vocabulario.

Sin embargo, tomemos una oración dada en el contexto de planning (en particular
del dominio \emph{satellite} descripto en apéndice A). La siguiente secuencia
muestra un plan relajado de largo 4 asociado a una tarea STRIPS cuyos esquemas
de acción son $calibrate, switch\_on, take\_image,$ y $turn\_to$. El resto de
expresiones son objetos concretos del dominio. También es importante mencionar
que cada objeto de un cierto tipo está enumerado. Por ejemplo el objeto
$instrument1$ es del tipo $instrument$ cuyo índice es $1$. 

\begin{align*}
    & (calibrate\ satellite0\ instrument1\ groundstation0) \\
    & (switch\_on\ instrument1\ satellite0) \\
    & (take\_image\ satellite0\ planet5\ instrument1\ image1) \\
    & (turn\_to\ satellite0\ groundstation0\ planet5) \\
\end{align*}

La primera dificultad para codificarla surge bajo la pregunta de a que se
identifica como una palabra de la oración. Una primera iniciativa es definir una
acción como un token y utilizar un one hot encoding. No obstante, por lo general
una acción no suele ocurrir más de una vez en un plan de la tarea lo cual se
perdería la información de la frecuencia en que ocurren los objetos en la
secuencia. No solo es clave capturar los objetos si no también el índice del
cual se trata. Por lo tanto respetar su numeración es igual de relevante. Por
último cada acción debe mantener la misma cantidad de componentes en su
representación vectorial, independientemente del esquema o el número de
parámetros que reciba, y se debe asegurar el orden de la secuencia.

Para solventar estas dificultades se mantuvo un codificación one hot ordinal que
consiste en:

\begin{itemize}
    \item Cada elemento en la interfaz de una acción junto a su esquema son
    definidos como tokens. Eso incluye la numeración de los objetos.
    \item Cada vector que representa a una acción tiene dimensión $2 \times N +
    1$ siendo $N$ la longitud de la interfaz más larga de un esquema. Aquellas
    acciones con una interfaz más chica se les agrega un margen de $0's$ hasta
    completar el largo requerido.
    \item Los esquemas de acción son enumerados en el rango $1, ..., M$ con $M$
    la cantidad de esquemas.
    \item El tipo de los objetos son enumerados en el rango de $1, ..., K$. con
    $K$ la cantidad de tipos.
    \item Si un objeto tiene el índice $i$ se lo incrementa en 1 (para evitar que
    aquellos objetos que empiecen con indice 0 se malinterpreten como margenes).
\end{itemize}

Por ejemplo si la enumeración de las esquemas y tipos fuesen los siguientes:

\begin{align*}
    &\{calibrate: 1,\ turn\_to: 2,\ switch\_on:3,\ take\_image:4 \} \\
    &\{satellite: 1,\ instrument: 2,\ planet: 3,\ groundstation: 4,\ image: 5\} \\
    & Longitud\ interfaz\ más\ larga = 4
\end{align*}

Entonces cada acción tendrá asociado un vector de dimensión $2 \times 4 + 1 = 9$
y la codificación resultante sería:

\begin{align*}
    (calibrate\ satellite0\ instrument1\ groundstation0) &: (1\ 1\ 1\ 2\ 2\ 4\ 1\ 0\ 0) \\
    (switch\_on\ instrument1\ satellite0) &: (3\ 2\ 2\ 1\ 1\ 0\ 0\ 0\ 0) \\
    (take\_image\ satellite0\ planet5\ instrument1\ image1) &: (4\ 1\ 1\ 3\ 6\ 2\ 2\ 5\ 2) \\
    (turn\_to\ satellite0\ groundstation0\ planet5) &: (2\ 1\ 1\ 4\ 1\ 3\ 6\ 0\ 0) \\
\end{align*}





\subsection{Word embeddings (vectores de palabras)}

Un vector denso de palabras (comúnmente conocido como word
embedding, en inglés) es una de las técnicas de representación de vo-
cabulario más popular en el área del procesamiento de lenguaje natural.

Si bien hay distintas formas de representar palabras en una serie de documentos,
los word embeddings proveen otra perspectiva que busca encontrar una
representación vectorial compacta donde cada dimensión logre capturar las
propiedades subyacentes y latentes de la palabra (sintácticas o semánticas). De
esta manera, los embeddings son superiores a una representación que permanece en
un nivel poco profundo

Es capaz de capturar el contexto, semántica, y similitud sintáctica de una
palabra en un documento

\section{Entrenamiento y evaluación}

\subsection{Validación cruzada}

\subsection{Búsqueda de hiperparámetros}

\section{Algoritmos de clasificación}

\subsection{Regresión logistica}

\subsection{XGBoost}

\subsection{Redes neuronales}

\section{Métricas de clasificación}